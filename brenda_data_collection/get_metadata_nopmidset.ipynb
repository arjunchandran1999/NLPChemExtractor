{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantifying Datasets\n",
    "Gauge the coverage for various sources for FULL TEXT papers.\n",
    "\n",
    "List of sources to check:\n",
    "1. Unpaywall (includes pmid?)\n",
    "2. BioArxiv\n",
    "3. ChemArxiv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Unpaywall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will try using missing pmid's (use title query). This is in get_doi/refDict.p."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import requests\n",
    "import os\n",
    "import math\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "email = 'ivalexander13@berkeley.edu' # enter ur email pls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import cPickle as pickle\n",
    "except ImportError:  # Python 3.x\n",
    "    import pickle\n",
    "    \n",
    "with open('./get_doi/refDict.p', 'rb') as fp:\n",
    "    refDict = pickle.load(fp)\n",
    "    \n",
    "# refDict is ready to use now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pasted get_doi from get_doi.ipynb. This is the first way of querying. Modified to work with value insteaad of key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lxml.etree as ET\n",
    "import urllib\n",
    "import re\n",
    "\n",
    "def get_doi(metadata: dict):\n",
    "    DOIerrors = {}\n",
    "    data = metadata\n",
    "#     for i in ([\"journal\", \"volume\", \"year\",\"pages\"]): # why is this here?\n",
    "#     print(data)\n",
    "    \n",
    "    journal = \" \".join(data[\"journal\"].split())\n",
    "    volume = \"\".join(data[\"volume\"].split())\n",
    "    year = \"\".join(data[\"year\"].split())\n",
    "    start_page = \"\".join(data[\"pages\"].split()).split(\"-\")[0]\n",
    "    first_author_last_name = \"\".join(data[\"authors\"].split(\";\")[0].split(\",\")[0].split())\n",
    "    journal_title = journal.replace(\" \", '%20')\n",
    "    params = (first_author_last_name, journal_title, volume, start_page, year)\n",
    "    url = \"https://doi.crossref.org/openurl?pid=mrunali@berkeley.edu&aulast={}&title={}&volume={}&spage={}&date={}&noredirect=true\".format(*params)\n",
    "\n",
    "    data = urllib.request.urlopen(url).read()\n",
    "#     print(data)\n",
    "#     print(url)\n",
    "    data = data.decode(\"utf-8\")\n",
    "    \n",
    "#     print(li)\n",
    "\n",
    "    if (data.find('unresolved') != -1):\n",
    "        DOIerrors[key] = data\n",
    "        return \n",
    "    else:\n",
    "        try:\n",
    "            doi = re.findall('(<doi.*>)(.*)<\\/doi>', data)[0][1]\n",
    "#             return \"https://sci-hub.tw/{}\".format(doi)\n",
    "            return doi\n",
    "        except Exception as e:\n",
    "            DOIerrors[key] = data\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query by metadata by doi. Input is the value items in refDict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns dict if found, else returns False.\n",
    "def query_meta_doi(key: str, value: dict):\n",
    "    doi = get_doi(value)\n",
    "    url = f'https://api.unpaywall.org/v2/{doi}?email={email}'\n",
    "    \n",
    "    out_dict = requests.get(url).json()\n",
    "    \n",
    "    if 'error' in out_dict:\n",
    "        return False\n",
    "    else:\n",
    "        return out_dict\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'doi': '10.1021/jo00059a013',\n",
       " 'doi_url': 'https://doi.org/10.1021/jo00059a013',\n",
       " 'title': 'An enantioselective synthesis of the topically-active carbonic anhydrase inhibitor MK-0507: 5,6-dihydro-(S)-4-(ethylamino)-(S)-6-methyl-4H-thieno[2,3-b]thiopyran-2-sulfonamide 7,7-dioxide hydrochloride',\n",
       " 'genre': 'journal-article',\n",
       " 'is_paratext': False,\n",
       " 'published_date': '1993-03-01',\n",
       " 'year': 1993,\n",
       " 'journal_name': 'The Journal of Organic Chemistry',\n",
       " 'journal_issns': '0022-3263,1520-6904',\n",
       " 'journal_issn_l': '0022-3263',\n",
       " 'journal_is_oa': False,\n",
       " 'journal_is_in_doaj': False,\n",
       " 'publisher': 'American Chemical Society (ACS)',\n",
       " 'is_oa': False,\n",
       " 'oa_status': 'closed',\n",
       " 'has_repository_copy': False,\n",
       " 'best_oa_location': None,\n",
       " 'first_oa_location': None,\n",
       " 'oa_locations': [],\n",
       " 'updated': '2020-09-10T11:51:33.841618',\n",
       " 'data_standard': 2,\n",
       " 'z_authors': [{'family': 'Blacklock',\n",
       "   'given': 'Thomas J.',\n",
       "   'sequence': 'first'},\n",
       "  {'family': 'Sohar', 'given': 'Paul', 'sequence': 'additional'},\n",
       "  {'family': 'Butcher', 'given': 'John W.', 'sequence': 'additional'},\n",
       "  {'family': 'Lamanec', 'given': 'T.', 'sequence': 'additional'},\n",
       "  {'family': 'Grabowski', 'given': 'E. J. J.', 'sequence': 'additional'}]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_meta_doi('659773', {\n",
    "    'authors': 'Blacklock, T.J.; Sohar, P.; Butcher, J.W.; Lamanec, T.; Grabowski, E.J.J.',\n",
    "    'title': 'An enantioselective synthesis of the topically-active carbonic anhydrase inhibitor MK-0507:5,6-dihydro-(s)-4-(ethylamino)-(s)-6-mehtyl-4H-thieno[2,3-beta]thiopyran-2-sulfonamide 7,7-dioxide hydrochloride',\n",
    "    'journal': 'J. Org. Chem.',\n",
    "    'volume': '58',\n",
    "    'pages': '1672-1679',\n",
    "    'year': '1993',\n",
    "    'pubmedId': '0',\n",
    "    'textmining': '0'\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each item in refDict, check if it has been successfully called before, then try to get its doi (get_doi). If doi not found, use title query instead. If still not found, then mark it as fail.\n",
    "\n",
    "Adjustable things:\n",
    "- max_calls: int or False (if we want to query all)\n",
    "- delete the query_out.json file if you want to reset the data\n",
    "- Interrupt the loop by clicking \"I\" twice (or Kernel>Interrupt). The data will be saved, and stats shown."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. check if json file is saved. Create or load a dict!!\n",
    "json_file = './quantify_datasets/query_out.json'\n",
    "if os.path.isfile(json_file):\n",
    "    with open(json_file, 'r') as fp:\n",
    "        query_out = json.load(fp)\n",
    "        fp.close()\n",
    "else:\n",
    "    query_out = {}\n",
    "    \n",
    "# stats\n",
    "max_calls = 90000 # max 100K per day (to be safe); or -1 if querying all.\n",
    "calls = 0\n",
    "queries = 0\n",
    "fails = {}\n",
    "are_oa = 0\n",
    "successes_or_found = 0\n",
    "\n",
    "# MAIN LOOP\n",
    "try:\n",
    "    # Looping through no-pmid papers (~12K)\n",
    "    for key, value in refDict.items():\n",
    "        key = str(math.floor(key))\n",
    "\n",
    "        # stop before maxing out query\n",
    "        if calls == max_calls or (queries == 99999):\n",
    "            print(\"Query limit reached.\")\n",
    "            break\n",
    "        else:\n",
    "            calls += 1\n",
    "\n",
    "        # 2. if not, then construct query\n",
    "        if key not in query_out:  # there are 7 None's\n",
    "            raw_title = value['title']\n",
    "            if raw_title is not None:\n",
    "                title = raw_title.strip()\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "\n",
    "            try:\n",
    "                # First: DOI Query\n",
    "                out_dict = query_meta_doi(key, value) # either dict or False\n",
    "                queries += 1\n",
    "\n",
    "                # if not found by doi, do title query. Errors if fails #fixme\n",
    "                if not out_dict:\n",
    "                    title_arg = title.replace(\" \", '%20')\n",
    "                    url = f'https://api.unpaywall.org/v2/search/?query={title_arg}&email={email}'\n",
    "                    out_dict = requests.get(url).json()['results'][0]['response']\n",
    "                    queries += 1\n",
    "\n",
    "\n",
    "                # 4. append to DICT if successful\n",
    "                query_out[key] = out_dict\n",
    "\n",
    "                print(f\"#### Call {calls} success\")\n",
    "\n",
    "                # Extra: note down # oa's\n",
    "                if out_dict['is_oa']:\n",
    "                    are_oa += 1\n",
    "\n",
    "                successes_or_found += 1 # only reached when either doi or title query is successful\n",
    "            except KeyboardInterrupt:\n",
    "                print(\"### INTERRUPTED\")\n",
    "                break\n",
    "            except:\n",
    "    #             url = f'https://api.unpaywall.org/v2/search/?query={title_arg}&email={email}'\n",
    "\n",
    "                fails[key] = [url, value]\n",
    "                print(f\"############# Calls: {calls}, Fails: {len(fails)}\")\n",
    "\n",
    "        else:\n",
    "            if query_out[key]['is_oa']:\n",
    "                    are_oa += 1\n",
    "            successes_or_found += 1\n",
    "            print(f\"Call {calls} found\")\n",
    "            \n",
    "    # Looping through the rest (100K?)\n",
    "            \n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "        \n",
    "# 5. save dict to file\n",
    "with open(json_file, 'w') as fp:\n",
    "    json.dump(query_out, fp)\n",
    "    # vary: alter frequency of file save\n",
    "    if (calls % 1 == 0):\n",
    "        fp.close()\n",
    "        \n",
    "# Print stats\n",
    "print(\"\")\n",
    "print(\"###### STATS ######\")\n",
    "print(f\"Total calls: {calls}\")\n",
    "print(f\"Total number of queries: {queries}\")\n",
    "print(f\"Number of failed queries: {len(fails)}\")\n",
    "print(f\"Number of open access papers: {are_oa}\")\n",
    "print(f\"Number of papers in storage: {successes_or_found}\")\n",
    "print(f\"% of open access papers: {are_oa / (successes_or_found+1) * 100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting PMID -> DOI -> Query (not refDict). \n",
    "~100K unique PMIDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all duplicates and zeroes. \n",
    "pmids = pd.read_csv(\"../Data_CSVs/brenda_rxns_incomplete.csv\").pubmedId\n",
    "pmids = pmids.drop_duplicates().dropna()\n",
    "pmids = pmids.apply(lambda x: str(math.floor(x)) if x != 0 else None)\n",
    "pmids = pmids.drop_duplicates().dropna()\n",
    "len(pmids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converts PMID to DOI using PubMed API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'10.1016/j.cbi.2008.10.037'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def doi_from_pmid(pmid):\n",
    "    url = f'https://www.ncbi.nlm.nih.gov/pmc/utils/idconv/v1.0/?tool=id_converter_api&email={email}&ids={pmid}'\n",
    "    data = requests.get(url).content.decode(\"utf-8\") \n",
    "    match =  re.findall('doi=\"([^\"]*)', data)\n",
    "    if match:\n",
    "        return match[0]\n",
    "    return False\n",
    "        \n",
    "doi_from_pmid(19022233)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CANT GET THIS TO WORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.pmid2cite.com/pmid-to-doi-converter'\n",
    "myobj = {'pmidNo': '11056675'}\n",
    "\n",
    "x = requests.post(url, data= myobj)\n",
    "\n",
    "print(x.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOOP AND QUERY\n",
    "\n",
    "# Check if json file is saved. Create or load a dict!!\n",
    "json_file = './quantify_datasets/query_out_pmid.json'\n",
    "if os.path.isfile(json_file):\n",
    "    with open(json_file, 'r') as fp:\n",
    "        query_out = json.load(fp)\n",
    "        fp.close()\n",
    "else:\n",
    "    query_out = {}\n",
    "    \n",
    "# stats\n",
    "pmid_max_calls = 100 #False #90000 # max 100K per day (to be safe); or False if querying all.\n",
    "pmid_calls = 0\n",
    "pmid_queries = 0\n",
    "pmid_fail_no_doi = [] # pmid strings\n",
    "pmid_fail_no_result = {} # pmid: [doi, url]\n",
    "pmid_are_oa = 0\n",
    "pmid_successes_or_found = 0\n",
    "    \n",
    "##### MAIN LOOP #####\n",
    "try: \n",
    "    for pmid in pmids:\n",
    "        # stop before maxing out query\n",
    "        if pmid_calls == pmid_max_calls or (pmid_queries == 99999):\n",
    "            print(\"Query limit reached.\")\n",
    "            break\n",
    "        else:\n",
    "            pmid_calls += 1\n",
    "            \n",
    "        # checks if pmid has been succcessful before.\n",
    "        if pmid in query_out:\n",
    "            successes_or_found += 1\n",
    "            print(f\"Call {pmid_calls} found\")\n",
    "            continue\n",
    "            \n",
    "        # query\n",
    "        if doi := doi_from_pmid(pmid):\n",
    "            url = f'https://api.unpaywall.org/v2/{doi}?email={email}'\n",
    "            out_dict = requests.get(url).json()\n",
    "            pmid_queries += 1\n",
    "            \n",
    "            #backup\n",
    "            if !out_dict:\n",
    "                url = 'https://www.w3schools.com/python/demopage.php'\n",
    "                myobj = {'somekey': 'somevalue'}\n",
    "\n",
    "                x = requests.post(url, data = myobj)\n",
    "\n",
    "                print(x.text)\n",
    "            \n",
    "            # 4. append to DICT if successful\n",
    "            if out_dict:\n",
    "                query_out[pmid] = out_dict\n",
    "                print(f\"## Call {pmid_calls} success\")\n",
    "                pmid_successes_or_found += 1 # only reached when either doi or title query is successful\n",
    "            else:\n",
    "                print(f\"####### Call {pmid_calls} failed. No results.\")\n",
    "\n",
    "            # Extra: note down # oa's\n",
    "            if out_dict['is_oa']:\n",
    "                pmid_are_oa += 1\n",
    "\n",
    "        else: # if doi isnt available\n",
    "            print(f\"#### Call {pmid_calls} failed. No DOI found.\")\n",
    "            pmid_fail_no_doi.append(pmid)\n",
    "            \n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "\n",
    "# 5. save dict to file\n",
    "with open(json_file, 'w') as fp:\n",
    "    json.dump(query_out, fp)\n",
    "    # vary: alter frequency of file save\n",
    "    if (calls % 1 == 0):\n",
    "        fp.close()\n",
    "        \n",
    "# Print stats\n",
    "print(\"\")\n",
    "print(\"###### STATS ######\")\n",
    "print(f\"Total calls: {pmid_calls}\")\n",
    "print(f\"Total number of queries: {pmid_queries}\")\n",
    "print(f\"Number of failed doi lookups: {len(pmid_fail_no_doi)}\")\n",
    "print(f\"Number of failed queries: {len(pmid_fail_no_result)}\")\n",
    "print(f\"Number of open access papers: {pmid_are_oa}\")\n",
    "print(f\"Number of papers in storage: {pmid_successes_or_found}\")\n",
    "print(f\"% of open access papers: {pmid_are_oa / (pmid_successes_or_found+1) * 100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmid_fail_no_doi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## See publisher stats for Elsevier API (worth it? yes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can be run without running all the cells above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file = './quantify_datasets/query_out.json'\n",
    "if os.path.isfile(json_file):\n",
    "    with open(json_file, 'r') as fp:\n",
    "        query_out = json.load(fp)\n",
    "        fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "publishers = {}\n",
    "are_oa = 0\n",
    "for key, value in query_out.items():\n",
    "    pub = value['publisher']\n",
    "    is_oa = value['is_oa']\n",
    "    \n",
    "    # Check by publisher\n",
    "    if True:\n",
    "        if pub not in publishers:\n",
    "                publishers[pub] = 1\n",
    "        else:\n",
    "            publishers[pub] += 1\n",
    "        \n",
    "    # check if paper is oa\n",
    "    if is_oa:\n",
    "        are_oa += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Elsevier papers in total: 4387.\n",
      "Total papers that we have = elsevier + OA = 4387 + 2127 = 6514\n",
      "% of full texts out of those found in Unpaywall (if all Elsevier papers found) = 6514 / 10398 = 62.646662819773034%\n",
      "% of full texts out of those found in TOTAL (12K) (if all Elsevier papers found) = 6514 / 12064 = 53.995358090185675%\n",
      "% if fetching from Elsevier + Springer + Wiley = 8534 / 12064 = 70.7393899204244%\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "sorted_x = sorted(publishers.items(), key=operator.itemgetter(1), reverse=True)\n",
    "sorted_x\n",
    "\n",
    "num_elsevier_papers = sum([v[1] for i, v in enumerate(sorted_x) if v[0] == 'Elsevier' or v[0] == 'Elsevier BV'])\n",
    "\n",
    "num_E_S_W = sum([v[1] for i, v in enumerate(sorted_x) if \n",
    "                 v[0] == 'Elsevier' \n",
    "                 or v[0] == 'Elsevier BV' \n",
    "                 or v[0] == 'Springer Science and Business Media LLC'\n",
    "                 or v[0] == 'Wiley'\n",
    "                ])\n",
    "\n",
    "\n",
    "# printouts\n",
    "print(f\"Number of Elsevier papers in total: {num_elsevier_papers}.\")\n",
    "print(f\"Total papers that we have = elsevier + OA = {num_elsevier_papers} + {are_oa} = {num_elsevier_papers + are_oa}\")\n",
    "print(f\"% of full texts out of those found in Unpaywall (if all Elsevier papers found) = {num_elsevier_papers + are_oa} / {len(query_out)} = {(num_elsevier_papers + are_oa) / len(query_out) * 100}%\")\n",
    "print(f\"% of full texts out of those found in TOTAL (12K) (if all Elsevier papers found) = {num_elsevier_papers + are_oa} / {len(refDict)} = {(num_elsevier_papers + are_oa) / len(refDict) * 100}%\")\n",
    "\n",
    "print(f\"% if fetching from Elsevier + Springer + Wiley = {num_E_S_W + are_oa} / {len(refDict)} = {(num_E_S_W + are_oa) / len(refDict) * 100}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Elsevier BV', 3654),\n",
       " ('Springer Science and Business Media LLC', 1214),\n",
       " ('Informa UK Limited', 894),\n",
       " ('Wiley', 806),\n",
       " ('Elsevier', 733),\n",
       " ('Portland Press Ltd.', 448),\n",
       " ('American Chemical Society (ACS)', 383),\n",
       " ('Oxford University Press (OUP)', 276),\n",
       " ('Microbiology Society', 195),\n",
       " ('Royal Society of Chemistry (RSC)', 189)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_x[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
