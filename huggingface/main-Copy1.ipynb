{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HuggingFace Transformers on SciBERT (NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import csv\n",
    "import re\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained('allenai/scibert_scivocab_uncased')\n",
    "# model = transformers.AutoModelForTokenClassification.from_pretrained('allenai/scibert_scivocab_uncased', num_labels=len(label_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An overview of the data pre-processing pipeline:\n",
    "1. txt files (train, test, dev)\n",
    "2. primary data (X_primary) =  dict of (id: list), (token: list), and (ner_tag: list) for every sentence; grouped by key (not sentence).\n",
    "3. tokenized data (X_tokenized) = 'dict' of token_id, label, and attention_mask; grouped by key.\n",
    "4. Dataset objects (X_data) = a 'list' of token_id, label, and attention mask; grouped by sentence!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input/Output Args\n",
    "DATA_DIR: str = \"./data/ner_chemprot/\"\n",
    "DATA_FILES: dict = {\n",
    "    \"train\": DATA_DIR + 'train.txt', \n",
    "    \"test\": DATA_DIR + 'test.txt', \n",
    "    \"val\": DATA_DIR + 'dev.txt'\n",
    "}\n",
    "label_list = ['O',\n",
    "          'B-enzyme',\n",
    "          'B-SUBSTRATE',\n",
    "          'I-SUBSTRATE',\n",
    "          'B-PRODUCT-OF',\n",
    "          'I-enzyme',\n",
    "          'I-PRODUCT-OF'\n",
    "         ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1-2 Loading the chemprot data from SciBERT into Primary Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def txt2primary(fname) -> OrderedDict:\n",
    "    # initialize primary data dict\n",
    "    primary_data = OrderedDict()\n",
    "    primary_data['id'] =  []\n",
    "    primary_data['tokens'] = []\n",
    "    primary_data['ner_tags'] = []\n",
    "    \n",
    "#     fname = DATA_DIR + fi #'head.txt' # to test with 2 sentences only.\n",
    "    \n",
    "    sentence_id = 0\n",
    "    with open(fname, \"r\") as f:\n",
    "        rd = csv.reader(f, delimiter='\\t')\n",
    "        \n",
    "        is_blank_after_docstart = False\n",
    "        tmp_words = []\n",
    "        tmp_ners = []\n",
    "        for row in rd:\n",
    "            if is_blank_after_docstart:\n",
    "                is_blank_after_docstart = False\n",
    "                continue\n",
    "            elif not row:\n",
    "                continue\n",
    "            elif re.findall('DOCSTART', row[0]):\n",
    "                is_blank_after_docstart = True\n",
    "                continue\n",
    "            elif row[0] == '.' and row[1] == '.': # currently doesn't include periods.\n",
    "\n",
    "                primary_data['id'].extend([sentence_id])\n",
    "                primary_data['tokens'].extend([tmp_words])\n",
    "                primary_data['ner_tags'].extend([tmp_ners])\n",
    "\n",
    "                \n",
    "                sentence_id += 1\n",
    "                tmp_words = []\n",
    "                tmp_ners = []\n",
    "                continue\n",
    "                \n",
    "            tmp_words += [row[0]]\n",
    "            tmp_ners += [label_list.index(row[3])]\n",
    "    return primary_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_primary: OrderedDict = txt2primary(DATA_FILES['train'])\n",
    "val_primary: OrderedDict = txt2primary(DATA_FILES['val'])\n",
    "test_primary: OrderedDict = txt2primary(DATA_FILES['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 0,\n",
       " 'tokens': ['The',\n",
       "  'enzyme',\n",
       "  'cyclo-oxygenase',\n",
       "  'catalyses',\n",
       "  'the',\n",
       "  'oxygenation',\n",
       "  'of',\n",
       "  'arachidonic',\n",
       "  'acid',\n",
       "  ',',\n",
       "  'leading',\n",
       "  'to',\n",
       "  'the',\n",
       "  'formation',\n",
       "  'of',\n",
       "  'prostaglandins'],\n",
       " 'ner_tags': [0, 0, 1, 0, 0, 0, 0, 2, 3, 0, 0, 0, 0, 0, 0, 4]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_entry(i, primary_data: OrderedDict) -> dict:\n",
    "    out = {\n",
    "        'id': primary_data['id'][i],\n",
    "        'tokens': primary_data['tokens'][i],\n",
    "        'ner_tags': primary_data['ner_tags'][i]\n",
    "    }\n",
    "    return out\n",
    "\n",
    "example = get_entry(0, train_primary)\n",
    "example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2-3 Tokenize the Primary Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then tokenize the primary data to get their encodings, and create a Dataset object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verifying the tokenizer, based on the reference notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'the', 'enzyme', 'cyclo', '-', 'oxygen', '##ase', 'cataly', '##ses', 'the', 'oxygenation', 'of', 'arachid', '##onic', 'acid', ',', 'leading', 'to', 'the', 'formation', 'of', 'prostaglandin', '##s', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "tokenized_input = tokenizer(example[\"tokens\"], is_split_into_words=True)\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 24)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(example[f\"ner_tags\"]), len(tokenized_input[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24 24\n"
     ]
    }
   ],
   "source": [
    "word_ids = tokenized_input.word_ids()\n",
    "aligned_labels = [-100 if i is None else example[f\"ner_tags\"][i] for i in word_ids]\n",
    "print(len(aligned_labels), len(tokenized_input[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define and apply the tokenization function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_all_tokens = True\n",
    "def tokenize_and_align_labels(primary) -> transformers.tokenization_utils_base.BatchEncoding: # basically dict\n",
    "    tokenized_inputs = tokenizer(primary[\"tokens\"], \n",
    "                                 padding=True, \n",
    "                                 truncation=True, \n",
    "                                 is_split_into_words=True,\n",
    "                                 return_token_type_ids=False\n",
    "                                )\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(primary[f\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n",
    "            # ignored in the loss function.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            # We set the label for the first token of each word.\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
    "            # the label_all_tokens flag.\n",
    "            else:\n",
    "                label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        labels.append(label_ids)\n",
    "        \n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "# create tokenized inputs\n",
    "train_tokenized = tokenize_and_align_labels(train_primary)\n",
    "test_tokenized = tokenize_and_align_labels(test_primary)\n",
    "val_tokenized = tokenize_and_align_labels(val_primary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3-4 Create Dataset Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "        self.labels = encodings['labels']\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = list(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = Dataset(train_tokenized)\n",
    "test_data = Dataset(test_tokenized)\n",
    "val_data = Dataset(val_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Fine Tuning + Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overview of fine tuning and training:\n",
    "1. Metrics function\n",
    "2. Training\n",
    "3. Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Metrics Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PRODUCT-OF': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1},\n",
       " 'SUBSTRATE': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1},\n",
       " 'enzyme': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1},\n",
       " 'overall_precision': 1.0,\n",
       " 'overall_recall': 1.0,\n",
       " 'overall_f1': 1.0,\n",
       " 'overall_accuracy': 1.0}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datasets\n",
    "metric = datasets.load_metric(\"seqeval\")\n",
    "labels = [label_list[i] for i in example[f\"ner_tags\"]]\n",
    "metric.compute(predictions=[labels], references=[labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p) -> dict:\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining necessary functions as args."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model_init for hyperparameter search \n",
    "# ref: https://huggingface.co/blog/ray-tune\n",
    "def model_init():\n",
    "    return transformers.AutoModelForTokenClassification.from_pretrained('allenai/scibert_scivocab_uncased', num_labels=len(label_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/scibert_scivocab_uncased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at allenai/scibert_scivocab_uncased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='12870' max='12870' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12870/12870 1:54:40, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Product-of</th>\n",
       "      <th>Substrate</th>\n",
       "      <th>Enzyme</th>\n",
       "      <th>Overall Precision</th>\n",
       "      <th>Overall Recall</th>\n",
       "      <th>Overall F1</th>\n",
       "      <th>Overall Accuracy</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.260500</td>\n",
       "      <td>0.123251</td>\n",
       "      <td>{'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 48}</td>\n",
       "      <td>{'precision': 0.34615384615384615, 'recall': 0.11538461538461539, 'f1': 0.17307692307692307, 'number': 156}</td>\n",
       "      <td>{'precision': 0.6, 'recall': 0.05172413793103448, 'f1': 0.09523809523809525, 'number': 174}</td>\n",
       "      <td>0.397059</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.121076</td>\n",
       "      <td>0.952054</td>\n",
       "      <td>28.426800</td>\n",
       "      <td>10.237000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.120800</td>\n",
       "      <td>0.181756</td>\n",
       "      <td>{'precision': 0.7222222222222222, 'recall': 0.2708333333333333, 'f1': 0.39393939393939387, 'number': 48}</td>\n",
       "      <td>{'precision': 0.6739130434782609, 'recall': 0.1987179487179487, 'f1': 0.306930693069307, 'number': 156}</td>\n",
       "      <td>{'precision': 0.6666666666666666, 'recall': 0.2988505747126437, 'f1': 0.4126984126984128, 'number': 174}</td>\n",
       "      <td>0.676056</td>\n",
       "      <td>0.253968</td>\n",
       "      <td>0.369231</td>\n",
       "      <td>0.960374</td>\n",
       "      <td>25.653800</td>\n",
       "      <td>11.343000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.074500</td>\n",
       "      <td>0.249023</td>\n",
       "      <td>{'precision': 0.6363636363636364, 'recall': 0.14583333333333334, 'f1': 0.23728813559322035, 'number': 48}</td>\n",
       "      <td>{'precision': 0.6923076923076923, 'recall': 0.11538461538461539, 'f1': 0.19780219780219782, 'number': 156}</td>\n",
       "      <td>{'precision': 0.66, 'recall': 0.1896551724137931, 'f1': 0.29464285714285715, 'number': 174}</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.153439</td>\n",
       "      <td>0.249462</td>\n",
       "      <td>0.956734</td>\n",
       "      <td>24.994400</td>\n",
       "      <td>11.643000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.050300</td>\n",
       "      <td>0.190633</td>\n",
       "      <td>{'precision': 0.6896551724137931, 'recall': 0.4166666666666667, 'f1': 0.5194805194805195, 'number': 48}</td>\n",
       "      <td>{'precision': 0.5916666666666667, 'recall': 0.4551282051282051, 'f1': 0.5144927536231884, 'number': 156}</td>\n",
       "      <td>{'precision': 0.6329113924050633, 'recall': 0.28735632183908044, 'f1': 0.3952569169960474, 'number': 174}</td>\n",
       "      <td>0.618421</td>\n",
       "      <td>0.373016</td>\n",
       "      <td>0.465347</td>\n",
       "      <td>0.961726</td>\n",
       "      <td>25.569900</td>\n",
       "      <td>11.381000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.017700</td>\n",
       "      <td>0.223950</td>\n",
       "      <td>{'precision': 0.7333333333333333, 'recall': 0.4583333333333333, 'f1': 0.5641025641025641, 'number': 48}</td>\n",
       "      <td>{'precision': 0.363013698630137, 'recall': 0.33974358974358976, 'f1': 0.3509933774834437, 'number': 156}</td>\n",
       "      <td>{'precision': 0.46308724832214765, 'recall': 0.39655172413793105, 'f1': 0.42724458204334365, 'number': 174}</td>\n",
       "      <td>0.443077</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>0.409673</td>\n",
       "      <td>0.954342</td>\n",
       "      <td>28.558700</td>\n",
       "      <td>10.190000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.018100</td>\n",
       "      <td>0.241416</td>\n",
       "      <td>{'precision': 0.7666666666666667, 'recall': 0.4791666666666667, 'f1': 0.5897435897435898, 'number': 48}</td>\n",
       "      <td>{'precision': 0.5803571428571429, 'recall': 0.4166666666666667, 'f1': 0.4850746268656717, 'number': 156}</td>\n",
       "      <td>{'precision': 0.5754716981132075, 'recall': 0.3505747126436782, 'f1': 0.43571428571428567, 'number': 174}</td>\n",
       "      <td>0.600806</td>\n",
       "      <td>0.394180</td>\n",
       "      <td>0.476038</td>\n",
       "      <td>0.963495</td>\n",
       "      <td>30.688500</td>\n",
       "      <td>9.482000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.007100</td>\n",
       "      <td>0.277382</td>\n",
       "      <td>{'precision': 0.7857142857142857, 'recall': 0.4583333333333333, 'f1': 0.5789473684210527, 'number': 48}</td>\n",
       "      <td>{'precision': 0.4215686274509804, 'recall': 0.27564102564102566, 'f1': 0.33333333333333337, 'number': 156}</td>\n",
       "      <td>{'precision': 0.6082474226804123, 'recall': 0.3390804597701149, 'f1': 0.4354243542435424, 'number': 174}</td>\n",
       "      <td>0.546256</td>\n",
       "      <td>0.328042</td>\n",
       "      <td>0.409917</td>\n",
       "      <td>0.960166</td>\n",
       "      <td>28.022000</td>\n",
       "      <td>10.385000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.011800</td>\n",
       "      <td>0.298094</td>\n",
       "      <td>{'precision': 0.8333333333333334, 'recall': 0.4166666666666667, 'f1': 0.5555555555555556, 'number': 48}</td>\n",
       "      <td>{'precision': 0.7391304347826086, 'recall': 0.10897435897435898, 'f1': 0.18994413407821228, 'number': 156}</td>\n",
       "      <td>{'precision': 0.6701030927835051, 'recall': 0.3735632183908046, 'f1': 0.4797047970479705, 'number': 174}</td>\n",
       "      <td>0.708333</td>\n",
       "      <td>0.269841</td>\n",
       "      <td>0.390805</td>\n",
       "      <td>0.960790</td>\n",
       "      <td>27.035200</td>\n",
       "      <td>10.764000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.003400</td>\n",
       "      <td>0.249340</td>\n",
       "      <td>{'precision': 0.48, 'recall': 0.5, 'f1': 0.4897959183673469, 'number': 48}</td>\n",
       "      <td>{'precision': 0.625, 'recall': 0.5128205128205128, 'f1': 0.5633802816901409, 'number': 156}</td>\n",
       "      <td>{'precision': 0.5029585798816568, 'recall': 0.4885057471264368, 'f1': 0.4956268221574344, 'number': 174}</td>\n",
       "      <td>0.544669</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.521379</td>\n",
       "      <td>0.961102</td>\n",
       "      <td>25.894300</td>\n",
       "      <td>11.238000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.002800</td>\n",
       "      <td>0.259408</td>\n",
       "      <td>{'precision': 0.5102040816326531, 'recall': 0.5208333333333334, 'f1': 0.5154639175257733, 'number': 48}</td>\n",
       "      <td>{'precision': 0.36893203883495146, 'recall': 0.24358974358974358, 'f1': 0.29343629343629346, 'number': 156}</td>\n",
       "      <td>{'precision': 0.46774193548387094, 'recall': 0.5, 'f1': 0.4833333333333333, 'number': 174}</td>\n",
       "      <td>0.443787</td>\n",
       "      <td>0.396825</td>\n",
       "      <td>0.418994</td>\n",
       "      <td>0.953926</td>\n",
       "      <td>29.103300</td>\n",
       "      <td>9.999000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.001700</td>\n",
       "      <td>0.270660</td>\n",
       "      <td>{'precision': 0.6388888888888888, 'recall': 0.4791666666666667, 'f1': 0.5476190476190476, 'number': 48}</td>\n",
       "      <td>{'precision': 0.48936170212765956, 'recall': 0.4423076923076923, 'f1': 0.46464646464646464, 'number': 156}</td>\n",
       "      <td>{'precision': 0.5123456790123457, 'recall': 0.47701149425287354, 'f1': 0.494047619047619, 'number': 174}</td>\n",
       "      <td>0.516224</td>\n",
       "      <td>0.462963</td>\n",
       "      <td>0.488145</td>\n",
       "      <td>0.959230</td>\n",
       "      <td>25.292000</td>\n",
       "      <td>11.506000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.275857</td>\n",
       "      <td>{'precision': 0.6216216216216216, 'recall': 0.4791666666666667, 'f1': 0.5411764705882353, 'number': 48}</td>\n",
       "      <td>{'precision': 0.5118110236220472, 'recall': 0.4166666666666667, 'f1': 0.45936395759717313, 'number': 156}</td>\n",
       "      <td>{'precision': 0.5416666666666666, 'recall': 0.4482758620689655, 'f1': 0.49056603773584906, 'number': 174}</td>\n",
       "      <td>0.538961</td>\n",
       "      <td>0.439153</td>\n",
       "      <td>0.483965</td>\n",
       "      <td>0.960478</td>\n",
       "      <td>27.304000</td>\n",
       "      <td>10.658000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.278432</td>\n",
       "      <td>{'precision': 0.5, 'recall': 0.4791666666666667, 'f1': 0.48936170212765956, 'number': 48}</td>\n",
       "      <td>{'precision': 0.5793650793650794, 'recall': 0.46794871794871795, 'f1': 0.5177304964539007, 'number': 156}</td>\n",
       "      <td>{'precision': 0.5310344827586206, 'recall': 0.4425287356321839, 'f1': 0.4827586206896552, 'number': 174}</td>\n",
       "      <td>0.545741</td>\n",
       "      <td>0.457672</td>\n",
       "      <td>0.497842</td>\n",
       "      <td>0.960894</td>\n",
       "      <td>24.985700</td>\n",
       "      <td>11.647000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.281467</td>\n",
       "      <td>{'precision': 0.6216216216216216, 'recall': 0.4791666666666667, 'f1': 0.5411764705882353, 'number': 48}</td>\n",
       "      <td>{'precision': 0.5163398692810458, 'recall': 0.5064102564102564, 'f1': 0.511326860841424, 'number': 156}</td>\n",
       "      <td>{'precision': 0.5547945205479452, 'recall': 0.46551724137931033, 'f1': 0.5062500000000001, 'number': 174}</td>\n",
       "      <td>0.544643</td>\n",
       "      <td>0.484127</td>\n",
       "      <td>0.512605</td>\n",
       "      <td>0.961206</td>\n",
       "      <td>25.415000</td>\n",
       "      <td>11.450000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.281773</td>\n",
       "      <td>{'precision': 0.5476190476190477, 'recall': 0.4791666666666667, 'f1': 0.5111111111111111, 'number': 48}</td>\n",
       "      <td>{'precision': 0.5648854961832062, 'recall': 0.47435897435897434, 'f1': 0.5156794425087109, 'number': 156}</td>\n",
       "      <td>{'precision': 0.5374149659863946, 'recall': 0.4540229885057471, 'f1': 0.4922118380062306, 'number': 174}</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>0.465608</td>\n",
       "      <td>0.504298</td>\n",
       "      <td>0.961310</td>\n",
       "      <td>26.740100</td>\n",
       "      <td>10.883000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/scratch/ivalexander13/envs/hface/lib/python3.6/site-packages/transformers-4.4.2-py3.8.egg/transformers/data/data_collator.py:184: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/global/scratch/ivalexander13/envs/hface/lib/python3.6/site-packages/transformers-4.4.2-py3.8.egg/transformers/data/data_collator.py:191: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "Trainer is attempting to log a value of \"{'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 48}\" of type <class 'dict'> for key \"eval/PRODUCT-OF\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.34615384615384615, 'recall': 0.11538461538461539, 'f1': 0.17307692307692307, 'number': 156}\" of type <class 'dict'> for key \"eval/SUBSTRATE\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.6, 'recall': 0.05172413793103448, 'f1': 0.09523809523809525, 'number': 174}\" of type <class 'dict'> for key \"eval/enzyme\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "/global/scratch/ivalexander13/envs/hface/lib/python3.6/site-packages/transformers-4.4.2-py3.8.egg/transformers/data/data_collator.py:184: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/global/scratch/ivalexander13/envs/hface/lib/python3.6/site-packages/transformers-4.4.2-py3.8.egg/transformers/data/data_collator.py:191: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/global/scratch/ivalexander13/envs/hface/lib/python3.6/site-packages/transformers-4.4.2-py3.8.egg/transformers/data/data_collator.py:184: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/global/scratch/ivalexander13/envs/hface/lib/python3.6/site-packages/transformers-4.4.2-py3.8.egg/transformers/data/data_collator.py:191: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "Trainer is attempting to log a value of \"{'precision': 0.7222222222222222, 'recall': 0.2708333333333333, 'f1': 0.39393939393939387, 'number': 48}\" of type <class 'dict'> for key \"eval/PRODUCT-OF\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.6739130434782609, 'recall': 0.1987179487179487, 'f1': 0.306930693069307, 'number': 156}\" of type <class 'dict'> for key \"eval/SUBSTRATE\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.6666666666666666, 'recall': 0.2988505747126437, 'f1': 0.4126984126984128, 'number': 174}\" of type <class 'dict'> for key \"eval/enzyme\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "/global/scratch/ivalexander13/envs/hface/lib/python3.6/site-packages/transformers-4.4.2-py3.8.egg/transformers/data/data_collator.py:184: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/global/scratch/ivalexander13/envs/hface/lib/python3.6/site-packages/transformers-4.4.2-py3.8.egg/transformers/data/data_collator.py:191: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/global/scratch/ivalexander13/envs/hface/lib/python3.6/site-packages/transformers-4.4.2-py3.8.egg/transformers/data/data_collator.py:184: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/global/scratch/ivalexander13/envs/hface/lib/python3.6/site-packages/transformers-4.4.2-py3.8.egg/transformers/data/data_collator.py:191: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "Trainer is attempting to log a value of \"{'precision': 0.6363636363636364, 'recall': 0.14583333333333334, 'f1': 0.23728813559322035, 'number': 48}\" of type <class 'dict'> for key \"eval/PRODUCT-OF\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.6923076923076923, 'recall': 0.11538461538461539, 'f1': 0.19780219780219782, 'number': 156}\" of type <class 'dict'> for key \"eval/SUBSTRATE\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.66, 'recall': 0.1896551724137931, 'f1': 0.29464285714285715, 'number': 174}\" of type <class 'dict'> for key \"eval/enzyme\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "/global/scratch/ivalexander13/envs/hface/lib/python3.6/site-packages/transformers-4.4.2-py3.8.egg/transformers/data/data_collator.py:184: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/global/scratch/ivalexander13/envs/hface/lib/python3.6/site-packages/transformers-4.4.2-py3.8.egg/transformers/data/data_collator.py:191: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "Trainer is attempting to log a value of \"{'precision': 0.6896551724137931, 'recall': 0.4166666666666667, 'f1': 0.5194805194805195, 'number': 48}\" of type <class 'dict'> for key \"eval/PRODUCT-OF\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.5916666666666667, 'recall': 0.4551282051282051, 'f1': 0.5144927536231884, 'number': 156}\" of type <class 'dict'> for key \"eval/SUBSTRATE\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.6329113924050633, 'recall': 0.28735632183908044, 'f1': 0.3952569169960474, 'number': 174}\" of type <class 'dict'> for key \"eval/enzyme\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "/global/scratch/ivalexander13/envs/hface/lib/python3.6/site-packages/transformers-4.4.2-py3.8.egg/transformers/data/data_collator.py:184: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/global/scratch/ivalexander13/envs/hface/lib/python3.6/site-packages/transformers-4.4.2-py3.8.egg/transformers/data/data_collator.py:191: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/global/scratch/ivalexander13/envs/hface/lib/python3.6/site-packages/transformers-4.4.2-py3.8.egg/transformers/data/data_collator.py:184: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/global/scratch/ivalexander13/envs/hface/lib/python3.6/site-packages/transformers-4.4.2-py3.8.egg/transformers/data/data_collator.py:191: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "Trainer is attempting to log a value of \"{'precision': 0.7333333333333333, 'recall': 0.4583333333333333, 'f1': 0.5641025641025641, 'number': 48}\" of type <class 'dict'> for key \"eval/PRODUCT-OF\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.363013698630137, 'recall': 0.33974358974358976, 'f1': 0.3509933774834437, 'number': 156}\" of type <class 'dict'> for key \"eval/SUBSTRATE\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.46308724832214765, 'recall': 0.39655172413793105, 'f1': 0.42724458204334365, 'number': 174}\" of type <class 'dict'> for key \"eval/enzyme\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "/global/scratch/ivalexander13/envs/hface/lib/python3.6/site-packages/transformers-4.4.2-py3.8.egg/transformers/data/data_collator.py:184: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/global/scratch/ivalexander13/envs/hface/lib/python3.6/site-packages/transformers-4.4.2-py3.8.egg/transformers/data/data_collator.py:191: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/global/scratch/ivalexander13/envs/hface/lib/python3.6/site-packages/transformers-4.4.2-py3.8.egg/transformers/data/data_collator.py:184: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/global/scratch/ivalexander13/envs/hface/lib/python3.6/site-packages/transformers-4.4.2-py3.8.egg/transformers/data/data_collator.py:191: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "Trainer is attempting to log a value of \"{'precision': 0.7666666666666667, 'recall': 0.4791666666666667, 'f1': 0.5897435897435898, 'number': 48}\" of type <class 'dict'> for key \"eval/PRODUCT-OF\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.5803571428571429, 'recall': 0.4166666666666667, 'f1': 0.4850746268656717, 'number': 156}\" of type <class 'dict'> for key \"eval/SUBSTRATE\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.5754716981132075, 'recall': 0.3505747126436782, 'f1': 0.43571428571428567, 'number': 174}\" of type <class 'dict'> for key \"eval/enzyme\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "/global/scratch/ivalexander13/envs/hface/lib/python3.6/site-packages/transformers-4.4.2-py3.8.egg/transformers/data/data_collator.py:184: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/global/scratch/ivalexander13/envs/hface/lib/python3.6/site-packages/transformers-4.4.2-py3.8.egg/transformers/data/data_collator.py:191: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/global/scratch/ivalexander13/envs/hface/lib/python3.6/site-packages/transformers-4.4.2-py3.8.egg/transformers/data/data_collator.py:184: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/global/scratch/ivalexander13/envs/hface/lib/python3.6/site-packages/transformers-4.4.2-py3.8.egg/transformers/data/data_collator.py:191: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "Trainer is attempting to log a value of \"{'precision': 0.7857142857142857, 'recall': 0.4583333333333333, 'f1': 0.5789473684210527, 'number': 48}\" of type <class 'dict'> for key \"eval/PRODUCT-OF\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.4215686274509804, 'recall': 0.27564102564102566, 'f1': 0.33333333333333337, 'number': 156}\" of type <class 'dict'> for key \"eval/SUBSTRATE\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.6082474226804123, 'recall': 0.3390804597701149, 'f1': 0.4354243542435424, 'number': 174}\" of type <class 'dict'> for key \"eval/enzyme\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "/global/scratch/ivalexander13/envs/hface/lib/python3.6/site-packages/transformers-4.4.2-py3.8.egg/transformers/data/data_collator.py:184: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/global/scratch/ivalexander13/envs/hface/lib/python3.6/site-packages/transformers-4.4.2-py3.8.egg/transformers/data/data_collator.py:191: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "Trainer is attempting to log a value of \"{'precision': 0.8333333333333334, 'recall': 0.4166666666666667, 'f1': 0.5555555555555556, 'number': 48}\" of type <class 'dict'> for key \"eval/PRODUCT-OF\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.7391304347826086, 'recall': 0.10897435897435898, 'f1': 0.18994413407821228, 'number': 156}\" of type <class 'dict'> for key \"eval/SUBSTRATE\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.6701030927835051, 'recall': 0.3735632183908046, 'f1': 0.4797047970479705, 'number': 174}\" of type <class 'dict'> for key \"eval/enzyme\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "/global/scratch/ivalexander13/envs/hface/lib/python3.6/site-packages/transformers-4.4.2-py3.8.egg/transformers/data/data_collator.py:184: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/global/scratch/ivalexander13/envs/hface/lib/python3.6/site-packages/transformers-4.4.2-py3.8.egg/transformers/data/data_collator.py:191: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/global/scratch/ivalexander13/envs/hface/lib/python3.6/site-packages/transformers-4.4.2-py3.8.egg/transformers/data/data_collator.py:184: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/global/scratch/ivalexander13/envs/hface/lib/python3.6/site-packages/transformers-4.4.2-py3.8.egg/transformers/data/data_collator.py:191: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "Trainer is attempting to log a value of \"{'precision': 0.48, 'recall': 0.5, 'f1': 0.4897959183673469, 'number': 48}\" of type <class 'dict'> for key \"eval/PRODUCT-OF\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.625, 'recall': 0.5128205128205128, 'f1': 0.5633802816901409, 'number': 156}\" of type <class 'dict'> for key \"eval/SUBSTRATE\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.5029585798816568, 'recall': 0.4885057471264368, 'f1': 0.4956268221574344, 'number': 174}\" of type <class 'dict'> for key \"eval/enzyme\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "/global/scratch/ivalexander13/envs/hface/lib/python3.6/site-packages/transformers-4.4.2-py3.8.egg/transformers/data/data_collator.py:184: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/global/scratch/ivalexander13/envs/hface/lib/python3.6/site-packages/transformers-4.4.2-py3.8.egg/transformers/data/data_collator.py:191: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/global/scratch/ivalexander13/envs/hface/lib/python3.6/site-packages/transformers-4.4.2-py3.8.egg/transformers/data/data_collator.py:184: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/global/scratch/ivalexander13/envs/hface/lib/python3.6/site-packages/transformers-4.4.2-py3.8.egg/transformers/data/data_collator.py:191: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "Trainer is attempting to log a value of \"{'precision': 0.5102040816326531, 'recall': 0.5208333333333334, 'f1': 0.5154639175257733, 'number': 48}\" of type <class 'dict'> for key \"eval/PRODUCT-OF\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.36893203883495146, 'recall': 0.24358974358974358, 'f1': 0.29343629343629346, 'number': 156}\" of type <class 'dict'> for key \"eval/SUBSTRATE\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.46774193548387094, 'recall': 0.5, 'f1': 0.4833333333333333, 'number': 174}\" of type <class 'dict'> for key \"eval/enzyme\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "/global/scratch/ivalexander13/envs/hface/lib/python3.6/site-packages/transformers-4.4.2-py3.8.egg/transformers/data/data_collator.py:184: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/global/scratch/ivalexander13/envs/hface/lib/python3.6/site-packages/transformers-4.4.2-py3.8.egg/transformers/data/data_collator.py:191: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "Trainer is attempting to log a value of \"{'precision': 0.6388888888888888, 'recall': 0.4791666666666667, 'f1': 0.5476190476190476, 'number': 48}\" of type <class 'dict'> for key \"eval/PRODUCT-OF\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.48936170212765956, 'recall': 0.4423076923076923, 'f1': 0.46464646464646464, 'number': 156}\" of type <class 'dict'> for key \"eval/SUBSTRATE\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.5123456790123457, 'recall': 0.47701149425287354, 'f1': 0.494047619047619, 'number': 174}\" of type <class 'dict'> for key \"eval/enzyme\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "/global/scratch/ivalexander13/envs/hface/lib/python3.6/site-packages/transformers-4.4.2-py3.8.egg/transformers/data/data_collator.py:184: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/global/scratch/ivalexander13/envs/hface/lib/python3.6/site-packages/transformers-4.4.2-py3.8.egg/transformers/data/data_collator.py:191: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/global/scratch/ivalexander13/envs/hface/lib/python3.6/site-packages/transformers-4.4.2-py3.8.egg/transformers/data/data_collator.py:184: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/global/scratch/ivalexander13/envs/hface/lib/python3.6/site-packages/transformers-4.4.2-py3.8.egg/transformers/data/data_collator.py:191: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "Trainer is attempting to log a value of \"{'precision': 0.6216216216216216, 'recall': 0.4791666666666667, 'f1': 0.5411764705882353, 'number': 48}\" of type <class 'dict'> for key \"eval/PRODUCT-OF\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.5118110236220472, 'recall': 0.4166666666666667, 'f1': 0.45936395759717313, 'number': 156}\" of type <class 'dict'> for key \"eval/SUBSTRATE\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.5416666666666666, 'recall': 0.4482758620689655, 'f1': 0.49056603773584906, 'number': 174}\" of type <class 'dict'> for key \"eval/enzyme\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "/global/scratch/ivalexander13/envs/hface/lib/python3.6/site-packages/transformers-4.4.2-py3.8.egg/transformers/data/data_collator.py:184: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/global/scratch/ivalexander13/envs/hface/lib/python3.6/site-packages/transformers-4.4.2-py3.8.egg/transformers/data/data_collator.py:191: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/global/scratch/ivalexander13/envs/hface/lib/python3.6/site-packages/transformers-4.4.2-py3.8.egg/transformers/data/data_collator.py:184: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/global/scratch/ivalexander13/envs/hface/lib/python3.6/site-packages/transformers-4.4.2-py3.8.egg/transformers/data/data_collator.py:191: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "Trainer is attempting to log a value of \"{'precision': 0.5, 'recall': 0.4791666666666667, 'f1': 0.48936170212765956, 'number': 48}\" of type <class 'dict'> for key \"eval/PRODUCT-OF\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.5793650793650794, 'recall': 0.46794871794871795, 'f1': 0.5177304964539007, 'number': 156}\" of type <class 'dict'> for key \"eval/SUBSTRATE\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.5310344827586206, 'recall': 0.4425287356321839, 'f1': 0.4827586206896552, 'number': 174}\" of type <class 'dict'> for key \"eval/enzyme\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "/global/scratch/ivalexander13/envs/hface/lib/python3.6/site-packages/transformers-4.4.2-py3.8.egg/transformers/data/data_collator.py:184: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/global/scratch/ivalexander13/envs/hface/lib/python3.6/site-packages/transformers-4.4.2-py3.8.egg/transformers/data/data_collator.py:191: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/global/scratch/ivalexander13/envs/hface/lib/python3.6/site-packages/transformers-4.4.2-py3.8.egg/transformers/data/data_collator.py:184: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/global/scratch/ivalexander13/envs/hface/lib/python3.6/site-packages/transformers-4.4.2-py3.8.egg/transformers/data/data_collator.py:191: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "Trainer is attempting to log a value of \"{'precision': 0.6216216216216216, 'recall': 0.4791666666666667, 'f1': 0.5411764705882353, 'number': 48}\" of type <class 'dict'> for key \"eval/PRODUCT-OF\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.5163398692810458, 'recall': 0.5064102564102564, 'f1': 0.511326860841424, 'number': 156}\" of type <class 'dict'> for key \"eval/SUBSTRATE\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.5547945205479452, 'recall': 0.46551724137931033, 'f1': 0.5062500000000001, 'number': 174}\" of type <class 'dict'> for key \"eval/enzyme\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "/global/scratch/ivalexander13/envs/hface/lib/python3.6/site-packages/transformers-4.4.2-py3.8.egg/transformers/data/data_collator.py:184: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/global/scratch/ivalexander13/envs/hface/lib/python3.6/site-packages/transformers-4.4.2-py3.8.egg/transformers/data/data_collator.py:191: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "Trainer is attempting to log a value of \"{'precision': 0.5476190476190477, 'recall': 0.4791666666666667, 'f1': 0.5111111111111111, 'number': 48}\" of type <class 'dict'> for key \"eval/PRODUCT-OF\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.5648854961832062, 'recall': 0.47435897435897434, 'f1': 0.5156794425087109, 'number': 156}\" of type <class 'dict'> for key \"eval/SUBSTRATE\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.5374149659863946, 'recall': 0.4540229885057471, 'f1': 0.4922118380062306, 'number': 174}\" of type <class 'dict'> for key \"eval/enzyme\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=12870, training_loss=0.03518363530269991, metrics={'train_runtime': 6885.8891, 'train_samples_per_second': 1.869, 'total_flos': 1232636236860600.0, 'epoch': 15.0, 'init_mem_cpu_alloc_delta': 695462, 'init_mem_cpu_peaked_delta': 513666, 'train_mem_cpu_alloc_delta': 1238876, 'train_mem_cpu_peaked_delta': 105189103})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = transformers.TrainingArguments(\n",
    "    f\"test-ner\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    num_train_epochs=15,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_data,         # training dataset\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=transformers.DataCollatorForTokenClassification(tokenizer),\n",
    "    eval_dataset=val_data,             # evaluation dataset\n",
    "    compute_metrics=compute_metrics,\n",
    "    model_init=model_init,\n",
    "#     model=model\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log your results here: https://docs.google.com/spreadsheets/d/1jolvSI9tCqHZqBMtX1MAUjht2WuXyl_uFauhbvHMUtQ/edit?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='282' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [282/282 00:30]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions = trainer.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PRODUCT-OF': {'precision': 0.125,\n",
       "  'recall': 0.125,\n",
       "  'f1': 0.125,\n",
       "  'number': 32},\n",
       " 'SUBSTRATE': {'precision': 0.7261904761904762,\n",
       "  'recall': 0.5,\n",
       "  'f1': 0.5922330097087378,\n",
       "  'number': 122},\n",
       " 'enzyme': {'precision': 0.6054421768707483,\n",
       "  'recall': 0.4564102564102564,\n",
       "  'f1': 0.52046783625731,\n",
       "  'number': 195},\n",
       " 'overall_precision': 0.5855513307984791,\n",
       " 'overall_recall': 0.44126074498567336,\n",
       " 'overall_f1': 0.5032679738562091,\n",
       " 'overall_accuracy': 0.9629198008263588}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_metrics(predictions[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Hyperparameter search??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainer.hyperparameter_search(direction=\"maximize\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import DataLoader\n",
    "\n",
    "# device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# model.train()\n",
    "\n",
    "# torch.manual_seed(10)\n",
    "# BATCH_SIZE = 64\n",
    "# train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=False, drop_last=True)\n",
    "\n",
    "# optim = transformers.AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# for epoch in range(3):\n",
    "#     for i, batch in enumerate(train_loader):\n",
    "#         print(f'Doing epoch {epoch}, entries {i*BATCH_SIZE} to {(i+1)*BATCH_SIZE} out of {len(train_loader)}')\n",
    "#         optim.zero_grad()\n",
    "#         input_ids = batch['input_ids'].to(device)\n",
    "#         attention_mask = batch['attention_mask'].to(device)\n",
    "#         labels = batch['labels'].to(device)\n",
    "#         outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "#         loss = outputs[0]\n",
    "#         loss.backward()\n",
    "#         optim.step()\n",
    "\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.13 (transformers)",
   "language": "python",
   "name": "python3.6.13_transformers"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
